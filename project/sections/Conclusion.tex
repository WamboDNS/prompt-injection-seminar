\section{\textit{Conclusion}}
In this paper, we introduced a taxonomy for prompt injection attacks against Large Language Models to allow the classification and categorization of attacks into groups.
We characterize attacks by three dimensions: the attack vector, the mode of operation, and the goal of the attack. 
Each of these dimensions is then further split up into subcategories, allowing a fine-grained categorization of prompt injection attacks.
This approach allows the grouping of PI attacks by different characteristics.
Additionally, our taxonomy can be used to look for defense mechanisms that might defend against whole subcategories of attacks instead of single attacks, making it easier to secure the model against malicious intent. 
To assist this defensive process, we directly present several defense mechanisms that can help mitigate the risks emitted from our introduced categories.
At the same time, we also highlight the need for further research in terms of defending against such attacks, as this is not only a challenge to the LLM, but also to us humans because attackers develop attacks at a rapid pace, making it harder to successfully block adversaries.
